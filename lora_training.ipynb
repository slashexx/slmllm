{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb3bfbf2",
   "metadata": {},
   "source": [
    "# üöÄ Automated LoRA Training Pipeline\n",
    "\n",
    "This notebook automatically trains a LoRA adapter for your selected model using your custom dataset.\n",
    "\n",
    "**Steps:**\n",
    "1. Install required packages\n",
    "2. Load your dataset from the backend\n",
    "3. Configure LoRA parameters\n",
    "4. Train the model\n",
    "5. Save and upload the trained adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f71323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes trl einops requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a3b0c",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration\n",
    "\n",
    "**Important:** If you clicked the Colab link from the training interface, the job parameters should be automatically configured. Otherwise, you'll need to set them manually in the next cell.\n",
    "\n",
    "You can also manually edit `JOB_ID` and `MODEL_NAME` in the configuration cell if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbefda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìù Manual Configuration (Optional)\n",
    "# If you're running this manually, update these values:\n",
    "\n",
    "# Paste your Job ID from the training interface (or leave as auto-generated)\n",
    "MANUAL_JOB_ID = \"\"  # Example: \"550e8400-e29b-41d4-a716-446655440000\"\n",
    "\n",
    "# Select your model (must match one of the available models)\n",
    "MANUAL_MODEL = \"tinyllama-1.1b\"  # Options: llama-2-7b, llama-2-13b, mistral-7b, phi-2, gemma-7b, tinyllama-1.1b\n",
    "\n",
    "# StarCoder Dataset Configuration (leave empty if using CSV upload)\n",
    "MANUAL_USE_STARCODER = False  # Set to True to use StarCoder dataset\n",
    "MANUAL_STARCODER_LANGUAGE = \"python\"  # Programming language (python, javascript, java, etc.)\n",
    "MANUAL_STARCODER_MAX_SAMPLES = 10000  # Maximum number of samples to use\n",
    "\n",
    "# If manual values are set, they will override auto-detection\n",
    "if MANUAL_JOB_ID:\n",
    "    print(f\"‚úì Using manual Job ID: {MANUAL_JOB_ID}\")\n",
    "if MANUAL_MODEL:\n",
    "    print(f\"‚úì Using manual model: {MANUAL_MODEL}\")\n",
    "if MANUAL_USE_STARCODER:\n",
    "    print(f\"‚úì Using StarCoder dataset: {MANUAL_STARCODER_LANGUAGE} (max {MANUAL_STARCODER_MAX_SAMPLES} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69752f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# Try to parse URL parameters from Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Will be set from job status API call\n",
    "USE_STARCODER = False\n",
    "STARCODER_LANGUAGE = None\n",
    "STARCODER_MAX_SAMPLES = 10000\n",
    "\n",
    "# Priority: Manual values > URL parameters > Environment variables > Auto-generated\n",
    "if 'MANUAL_JOB_ID' in globals() and MANUAL_JOB_ID:\n",
    "    JOB_ID = MANUAL_JOB_ID\n",
    "elif 'JOB_ID' in os.environ:\n",
    "    JOB_ID = os.environ['JOB_ID']\n",
    "else:\n",
    "    JOB_ID = ('colab' if IN_COLAB else 'local') + '-training-' + datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "if 'MANUAL_MODEL' in globals() and MANUAL_MODEL:\n",
    "    MODEL_NAME = MANUAL_MODEL\n",
    "elif 'MODEL_NAME' in os.environ:\n",
    "    MODEL_NAME = os.environ['MODEL_NAME']\n",
    "else:\n",
    "    MODEL_NAME = \"tinyllama-1.1b\"  # Default to smallest model for testing\n",
    "\n",
    "# StarCoder configuration - check manual config first\n",
    "if 'MANUAL_USE_STARCODER' in globals() and MANUAL_USE_STARCODER:\n",
    "    USE_STARCODER = True\n",
    "    STARCODER_LANGUAGE = MANUAL_STARCODER_LANGUAGE\n",
    "    STARCODER_MAX_SAMPLES = MANUAL_STARCODER_MAX_SAMPLES\n",
    "\n",
    "API_URL = \"https://slmllm-backend.vercel.app\"\n",
    "\n",
    "# Try to fetch job configuration from backend (includes StarCoder params)\n",
    "if JOB_ID and JOB_ID.startswith(('colab', 'local')):\n",
    "    # Auto-generated ID, skip API call\n",
    "    pass\n",
    "else:\n",
    "    try:\n",
    "        response = requests.get(f\"{API_URL}/api/train/status/{JOB_ID}\")\n",
    "        if response.status_code == 200:\n",
    "            job_config = response.json()\n",
    "            # Update model if not manually set\n",
    "            if 'MANUAL_MODEL' not in globals() or not MANUAL_MODEL:\n",
    "                if 'model' in job_config:\n",
    "                    MODEL_NAME = job_config['model']\n",
    "            # Check for StarCoder configuration\n",
    "            if job_config.get('dataset_type') == 'starcoder':\n",
    "                USE_STARCODER = True\n",
    "                STARCODER_LANGUAGE = job_config.get('starcoder_language', 'python')\n",
    "                STARCODER_MAX_SAMPLES = job_config.get('starcoder_max_samples', 10000)\n",
    "                print(f\"‚úì Loaded StarCoder config from job: {STARCODER_LANGUAGE} ({STARCODER_MAX_SAMPLES} samples)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not fetch job config from backend: {e}\")\n",
    "        print(\"   Using manual/environment configuration instead\")\n",
    "\n",
    "print(f\"üîß Configuration:\")\n",
    "print(f\"Job ID: {JOB_ID}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"API URL: {API_URL}\")\n",
    "print(f\"Environment: {'Google Colab' if IN_COLAB else 'Local'}\")\n",
    "if USE_STARCODER:\n",
    "    print(f\"üìä Dataset: StarCoder ({STARCODER_LANGUAGE}, max {STARCODER_MAX_SAMPLES} samples)\")\n",
    "else:\n",
    "    print(f\"üìä Dataset: CSV upload from backend\")\n",
    "print(\"\\n‚úÖ Configuration complete! Continue to the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cde201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model mapping\n",
    "MODEL_MAP = {\n",
    "    \"llama-2-7b\": \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"llama-2-13b\": \"meta-llama/Llama-2-13b-hf\",\n",
    "    \"mistral-7b\": \"mistralai/Mistral-7B-v0.1\",\n",
    "    \"phi-2\": \"microsoft/phi-2\",\n",
    "    \"gemma-7b\": \"google/gemma-7b\",\n",
    "    \"tinyllama-1.1b\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "}\n",
    "\n",
    "model_id = MODEL_MAP.get(MODEL_NAME, \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "print(f\"Using model: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3e0468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update training status\n",
    "def update_status(status, progress=0, error=None):\n",
    "    try:\n",
    "        data = {\n",
    "            \"status\": status,\n",
    "            \"progress\": progress\n",
    "        }\n",
    "        if error:\n",
    "            data[\"error\"] = error\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{API_URL}/api/train/update/{JOB_ID}\",\n",
    "            data=data\n",
    "        )\n",
    "        print(f\"Status update: {status} ({progress}%)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to update status: {e}\")\n",
    "\n",
    "# Update status to training\n",
    "update_status(\"training\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1c3563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset - either StarCoder from Hugging Face or CSV from backend\n",
    "if USE_STARCODER and STARCODER_LANGUAGE:\n",
    "    print(f\"üì• Loading StarCoder dataset from Hugging Face...\")\n",
    "    print(f\"Language: {STARCODER_LANGUAGE}, Max Samples: {STARCODER_MAX_SAMPLES}\")\n",
    "    \n",
    "    try:\n",
    "        from datasets import load_dataset\n",
    "        \n",
    "        # Load StarCoder dataset for the selected language\n",
    "        print(f\"Loading bigcode/starcoderdata dataset (this may take a few minutes)...\")\n",
    "        dataset = load_dataset(\"bigcode/starcoderdata\", data_dir=STARCODER_LANGUAGE, split=\"train\")\n",
    "        \n",
    "        # Limit to max_samples\n",
    "        if len(dataset) > STARCODER_MAX_SAMPLES:\n",
    "            print(f\"Limiting dataset from {len(dataset)} to {STARCODER_MAX_SAMPLES} samples...\")\n",
    "            dataset = dataset.select(range(STARCODER_MAX_SAMPLES))\n",
    "        \n",
    "        print(f\"‚úÖ Loaded {len(dataset)} code samples\")\n",
    "        \n",
    "        # Convert StarCoder format to training format\n",
    "        # StarCoder has 'content' field with code, we'll create instruction-response pairs\n",
    "        print(\"Converting StarCoder format to training format...\")\n",
    "        \n",
    "        def format_code_sample(example):\n",
    "            \"\"\"Format code sample as instruction-response pair\"\"\"\n",
    "            code = example.get('content', '')\n",
    "            # Create a simple instruction to complete/generate code\n",
    "            # You can customize this based on your needs\n",
    "            return {\n",
    "                \"input\": f\"Write a {STARCODER_LANGUAGE} code snippet:\",\n",
    "                \"output\": code[:2000]  # Limit code length to avoid issues\n",
    "            }\n",
    "        \n",
    "        # Convert to pandas DataFrame\n",
    "        formatted_data = dataset.map(format_code_sample)\n",
    "        df = pd.DataFrame({\n",
    "            'input': formatted_data['input'],\n",
    "            'output': formatted_data['output']\n",
    "        })\n",
    "        \n",
    "        print(f\"‚úÖ Converted to training format: {len(df)} examples\")\n",
    "        print(f\"\\nDataset preview:\")\n",
    "        print(df.head(3))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading StarCoder dataset: {e}\")\n",
    "        print(\"Falling back to sample dataset...\")\n",
    "        \n",
    "        sample_data = {\n",
    "            \"input\": [\n",
    "                \"Write a python function:\",\n",
    "                \"Write a python class:\",\n",
    "                \"Write a python script:\"\n",
    "            ],\n",
    "            \"output\": [\n",
    "                \"def hello_world():\\n    print('Hello, World!')\",\n",
    "                \"class MyClass:\\n    def __init__(self):\\n        self.value = 0\",\n",
    "                \"#!/usr/bin/env python3\\nprint('Hello from Python!')\"\n",
    "            ]\n",
    "        }\n",
    "        df = pd.DataFrame(sample_data)\n",
    "        print(f\"Created sample dataset with {len(df)} examples\")\n",
    "        \n",
    "else:\n",
    "    # Traditional CSV download from backend\n",
    "    print(\"üì• Downloading dataset from backend...\")\n",
    "    \n",
    "    try:\n",
    "        # Fetch the dataset file from the backend\n",
    "        response = requests.get(f\"{API_URL}/api/train/dataset/{JOB_ID}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Save the dataset locally\n",
    "            dataset_path = f\"training_dataset_{JOB_ID}.csv\"\n",
    "            with open(dataset_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            # Load the dataset\n",
    "            df = pd.read_csv(dataset_path)\n",
    "            print(f\"‚úÖ Successfully downloaded dataset: {len(df)} examples\")\n",
    "            print(f\"\\nDataset preview:\")\n",
    "            print(df.head())\n",
    "            \n",
    "            # Validate dataset format\n",
    "            if 'input' not in df.columns or 'output' not in df.columns:\n",
    "                raise ValueError(\"Dataset must have 'input' and 'output' columns\")\n",
    "            \n",
    "            print(f\"\\n‚úì Dataset columns: {list(df.columns)}\")\n",
    "            print(f\"‚úì Training examples: {len(df)}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Could not download dataset from backend (status: {response.status_code})\")\n",
    "            print(\"Using sample dataset for demonstration...\")\n",
    "            \n",
    "            # Fallback to sample dataset\n",
    "            sample_data = {\n",
    "                \"input\": [\n",
    "                    \"What is machine learning?\",\n",
    "                    \"Explain neural networks\",\n",
    "                    \"What is deep learning?\",\n",
    "                    \"What is supervised learning?\",\n",
    "                    \"Explain backpropagation\"\n",
    "                ],\n",
    "                \"output\": [\n",
    "                    \"Machine learning is a subset of AI that enables systems to learn from data without being explicitly programmed.\",\n",
    "                    \"Neural networks are computational models inspired by the human brain, consisting of interconnected nodes that process information.\",\n",
    "                    \"Deep learning is a subset of machine learning using multi-layered neural networks to learn hierarchical representations.\",\n",
    "                    \"Supervised learning is a machine learning approach where models learn from labeled training data to make predictions.\",\n",
    "                    \"Backpropagation is an algorithm for training neural networks by calculating gradients and updating weights to minimize error.\"\n",
    "                ]\n",
    "            }\n",
    "            df = pd.DataFrame(sample_data)\n",
    "            print(f\"Created sample dataset with {len(df)} examples\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading dataset: {e}\")\n",
    "        print(\"Using sample dataset for demonstration...\")\n",
    "        \n",
    "        sample_data = {\n",
    "            \"input\": [\n",
    "                \"What is machine learning?\",\n",
    "                \"Explain neural networks\",\n",
    "                \"What is deep learning?\",\n",
    "                \"What is supervised learning?\",\n",
    "                \"Explain backpropagation\"\n",
    "            ],\n",
    "            \"output\": [\n",
    "                \"Machine learning is a subset of AI that enables systems to learn from data without being explicitly programmed.\",\n",
    "                \"Neural networks are computational models inspired by the human brain, consisting of interconnected nodes that process information.\",\n",
    "                \"Deep learning is a subset of machine learning using multi-layered neural networks to learn hierarchical representations.\",\n",
    "                \"Supervised learning is a machine learning approach where models learn from labeled training data to make predictions.\",\n",
    "                \"Backpropagation is an algorithm for training neural networks by calculating gradients and updating weights to minimize error.\"\n",
    "            ]\n",
    "        }\n",
    "        df = pd.DataFrame(sample_data)\n",
    "        print(f\"Created sample dataset with {len(df)} examples\")\n",
    "\n",
    "update_status(\"training\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c802bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "from datasets import Dataset\n",
    "\n",
    "update_status(\"training\", 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f048974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 4-bit quantization for efficient training\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "print(f\"Loading {model_id}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "update_status(\"training\", 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d2ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "\n",
    "update_status(\"training\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a63e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for training\n",
    "def format_instruction(sample):\n",
    "    return f\"### Input:\\n{sample['input']}\\n\\n### Output:\\n{sample['output']}\"\n",
    "\n",
    "# Convert to HuggingFace dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(lambda x: {\"text\": format_instruction(x)})\n",
    "\n",
    "print(f\"Dataset prepared with {len(dataset)} examples\")\n",
    "update_status(\"training\", 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    warmup_steps=10,\n",
    "    max_grad_norm=0.3,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    ")\n",
    "\n",
    "print(\"Trainer configured successfully!\")\n",
    "update_status(\"training\", 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333a7cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"üöÄ Starting training...\")\n",
    "update_status(\"training\", 45)\n",
    "\n",
    "try:\n",
    "    trainer.train()\n",
    "    print(\"‚úÖ Training completed successfully!\")\n",
    "    update_status(\"training\", 90)\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {e}\")\n",
    "    update_status(\"failed\", 0, str(e))\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72796e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "output_dir = f\"./lora_model_{JOB_ID}\"\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to {output_dir}\")\n",
    "update_status(\"training\", 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4267fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "print(\"\\nüß™ Testing trained model...\")\n",
    "test_prompt = \"What is machine learning?\"\n",
    "inputs = tokenizer(format_instruction({\"input\": test_prompt, \"output\": \"\"}), return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\nPrompt: {test_prompt}\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the training job\n",
    "update_status(\"completed\", 100)\n",
    "print(\"\\n‚úÖ Training pipeline completed successfully!\")\n",
    "print(f\"\\nüì¶ Your trained LoRA adapter is ready in: {output_dir}\")\n",
    "print(\"\\nüí° To use this model:\")\n",
    "print(\"1. Download the adapter files from this Colab\")\n",
    "print(\"2. Load it with PEFT in your application\")\n",
    "print(\"3. Merge with base model or use directly\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
