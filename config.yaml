models:
  llm:
    provider: "ollama"
    model: "tinyllama"
    endpoint: "http://localhost:11434/api/chat"
    max_tokens: 4096
    cost_per_token: 0.0
  
  slm:
    provider: "ollama"
    model: "tinyllama"
    endpoint: "http://localhost:11434/api/chat"
    max_tokens: 2048
    cost_per_token: 0.0
  
  gemini:
    provider: "gemini"
    model: "gemini-2.5-flash"
    api_key: "AIzaSyBfIzIFgE6mDqjICEAcl5tK4eQrNpHAmXE"
    max_tokens: 4096
    cost_per_token: 0.00001

routing:
  complexity_threshold: 0.6
  max_slm_tokens: 500
  fallback_enabled: true
  cost_weight: 0.3
  latency_weight: 0.3
  quality_weight: 0.4

server:
  host: "0.0.0.0"
  port: 8000

